---
title: "Which AI model writes the best R code?"
author: "Sara Altman, Simon Couch"
execute: 
    echo: false
    warning: false
knitr: 
  opts_chunk:
    message: false
---

```{r libraries}
library(tidyverse)
library(gt)
library(ggrepel)
```

```{r setup}
# Load pre-processed data
app_data <- readr::read_rds(here::here("data/data_combined.rds"))

# Models to include in blog analysis
INCLUDED_MODELS <-
  c(
    "opus_4_5",
    "sonnet_4_5_thinking",
    "gpt_5",
    "gemini_3",
    "sonnet_4_thinking",
    "opus_4_1_thinking",
    "gpt_5_1"
  )

# Extract data from app_data
model_info <- app_data$model_info
are_eval <- app_data$eval_data |>
  filter(model_join %in% INCLUDED_MODELS) |>
  mutate(
    model = fct_recode(
      model_display,
      `Claude Sonnet 4\n(No Thinking)` = "Claude Sonnet 4 (No Thinking)"
    ),
    model = fct_reorder(
      model,
      score,
      .fun = \(x) sum(x == "Correct", na.rm = TRUE)
    )
  )

are_costs <-
  app_data$cost_data |>
  filter(model_join %in% INCLUDED_MODELS)

are_cost_summary <-
  are_costs |>
  summarize(
    min_input = min(input),
    max_input = max(input),
    min_output = min(output),
    max_output = max(output),
    avg_io_ratio = median(input / output),
    total_cost = sum(price)
  ) |>
  mutate(
    across(min_input:max_output, \(x) signif(x, 4) |> format(big.mark = ","))
  )
```

## Compare model performance in Shiny app

In a series of past blog posts, we evaluated how well various models generate R code. To do so, we used the [vitals package](https://vitals.tidyverse.org/), a framework for LLM evaluation. vitals contains functions for measuring the effectiveness of an LLM, as well as `are`, a dataset of challenging R coding problems and their solutions. We evaluated model performance on this set of coding problems. 

Since we started this series, there has been a proliferation of new models. **To help keep this evaluation up-to-date, we created a Shiny app to visualize results.** Going forward, we'll add new results to this app. The app lets you select and compare any of the models we've evaluated on both performance and cost. You can see the app [here](https://skaltman-model-eval-app.share.connect.posit.cloud). 

![](images/app.png)

We'll summarize current results in this post, but check out the app to create your own custom comparisons.

## Current recommendation: Claude Sonnet 4.5, Claude Opus 4.5, or OpenAI GPT-5

```{r main-plot, dpi=300}
are_eval |>
  ggplot(aes(y = model, fill = score)) +
  geom_bar(position = "fill") +
  scale_fill_manual(
    breaks = rev,
    values = c(
      "Correct" = "#6caea7",
      "Partially Correct" = "#f6e8c3",
      "Incorrect" = "#ef8a62"
    )
  ) +
  scale_x_continuous(labels = scales::percent, expand = c(5e-3, 5e-3)) +
  labs(
    x = "Percent",
    y = NULL,
    title = "Model performance on R code generation",
    fill = "Score"
  ) +
  theme_light() +
  theme(
    plot.subtitle = element_text(face = "italic"),
    legend.position = "bottom",
    plot.margin = margin(r = 10),
    axis.title = element_text(size = 14),
    title = element_text(size = 15),
    axis.text = element_text(size = 11.5),
    legend.text = element_text(size = 12)
  )
```

**For R coding tasks, we recommend using Claude Sonnet 4.5, Claude Opus 4.5, or OpenAI GPT-5.** Gemini 3 Pro, Google Gemini's latest model, also scored well. Notably, Claude Sonnet 4.5, released in September, shows marked improvements over version 4. 

```{r price-plot, dpi=300}
are_eval_summary <-
  are_eval |>
  group_by(model, model_join) |>
  summarize(percent_correct = sum(score == "Correct") / n()) |>
  ungroup() |>
  left_join(are_costs |> select(model_join, price), by = "model_join")

mean_correct <- mean(are_eval_summary$percent_correct)
mean_price <- mean(are_eval_summary$price)

are_eval_summary |>
  mutate(model = str_replace(model, "\n", " ")) |>
  ggplot(aes(price, percent_correct)) +
  geom_point() +
  annotate(
    "text",
    x = -0.15,
    y = 0.675,
    label = "High performing,\ninexpensive",
    hjust = 0,
    vjust = 0,
    color = "#666666",
    size = 4
  ) +
  annotate(
    "text",
    x = 6.5,
    y = 0.4,
    label = "Lower performing,\nexpensive",
    hjust = 1,
    vjust = 0,
    color = "#666666",
    size = 4
  ) +
  geom_hline(
    yintercept = mean_correct,
    color = "#666666",
    linewidth = 1,
    alpha = 0.4
  ) +
  geom_vline(
    xintercept = mean_price,
    color = "#666666",
    linewidth = 1,
    alpha = 0.4
  ) +
  geom_label_repel(
    aes(
      label = model,
      fill = (model %in% c("Claude Sonnet 4.5", "GPT-5", "Claude Opus 4.5"))
    ),
    force = 0.4,
    seed = 5,
    nudge_x = 0.2,
    color = "#333333",
    # fill = "#f5f5f5",
    size = 4.2
  ) +
  scale_x_continuous(labels = scales::label_dollar()) +
  scale_y_continuous(
    labels = scales::label_percent(),
    breaks = scales::breaks_width(0.05)
  ) +
  scale_fill_manual(values = c("#f5f5f5", "#e6edff")) +
  coord_cartesian(xlim = c(0, 6.3), ylim = c(0.4, 0.7)) +
  labs(
    x = "Total cost",
    y = "Percent correct",
    title = "Model performance on R code generation vs. cost",
    subtitle = "Cost reflects the actual total charges incurred."
  ) +
  theme_light() +
  theme(
    plot.subtitle = element_text(face = "italic"),
    legend.position = "none",
    axis.title = element_text(size = 14),
    title = element_text(size = 15),
    axis.text = element_text(size = 12)
  )
```

::: {.callout-caution}
## Take token usage into account
A **token** is the fundamental unit of data that an LLM can process (for text processing, a token is approximately a word). Different models use vastly different amounts of tokens. As a result, a model that is inexpensive on a per-token basis can, in practice, cost much more if it produces longer outputs. 

The above plot shows the actual costs incurred during the evaluation so it takes token count and per token cost into account. 
:::

## Key insights

* **Claude Opus 4.5, Claude Sonnet 4.5, and OpenAI GPT-5 are the current best performers on the set of R coding tasks.**

    These models represent some of the latest model releases from the major AI labs. 

    Claude Opus 4.5 (released November 2025) and Claude Sonnet 4.5 (released September 2025) show significant improvements over their predecessors. However, OpenAI's most recent release GPT-5.1 (November 2025) performed worse than GPT-5.^[[GPT-5.1 Codex](https://platform.openai.com/docs/models/gpt-5.1-codex), which is not shown here but can be seen in the [app](https://skaltman-model-eval-app.share.connect.posit.cloud), performed substantially better than GPT-5.1.]

* **Gemini 3, Google Gemini's latest model, also performed well.** It is slightly more expensive than our top three recommendations. 

* **Claude models show consistent improvement across versions**. Claude Sonnet 4.5 notably outperforms version 4 (released May 2025), and Opus 4.5 is cheaper and performed better than Opus 4.1. 

## Pricing

LLM pricing is typically provided per million tokens. In our evaluation process, each model used between `r are_cost_summary$min_input` and `r are_cost_summary$max_input` input tokens and between `r are_cost_summary$min_output` and `r are_cost_summary$max_output` output tokens. 

```{r pricing-table}
# Calculate summary stats for the table
are_eval_summary_table <-
  are_eval |>
  group_by(model, model_join) |>
  summarize(
    percent_correct = sum(score == "Correct") / n(),
    .groups = "drop"
  )

# Create table matching app format
are_eval_summary_table |>
  left_join(
    are_costs |> select(model_join, price, input, output),
    by = "model_join"
  ) |>
  left_join(model_info, by = "model_join") |>
  arrange(desc(percent_correct)) |>
  select(
    Model = model,
    `Input (per 1M tokens)` = Input,
    `Output (per 1M tokens)` = Output,
    `Input Tokens Used` = input,
    `Output Tokens Used` = output,
    `Total Cost` = price,
    `% Correct` = percent_correct
  ) |>
  mutate(Model = str_replace(Model, "\n", " ")) |>
  gt() |>
  fmt_currency(
    columns = c(
      `Input (per 1M tokens)`,
      `Output (per 1M tokens)`,
      `Total Cost`
    ),
    currency = "USD",
    decimals = 2
  ) |>
  fmt_number(
    columns = c(`Input Tokens Used`, `Output Tokens Used`),
    decimals = 0,
    use_seps = TRUE
  ) |>
  fmt_percent(
    columns = `% Correct`,
    decimals = 1
  ) |>
  cols_align(
    align = "left",
    columns = everything()
  ) |>
  tab_header(
    title = "Model Pricing and Performance Details",
    subtitle = "Sorted by percent correct (descending)"
  ) |>
  data_color(
    columns = `% Correct`,
    palette = c("#ef8a62", "#f6e8c3", "#6caea7"),
    domain = NULL
  ) |>
  data_color(
    columns = `Total Cost`,
    palette = c("#e8f4f8", "#a8d5e2", "#6baed6", "#3182bd", "#08519c"),
    domain = NULL
  )
```


## Methodology

* We used [ellmer](https://ellmer.tidyverse.org/) to create connections to the various models and [vitals](https://vitals.tidyverse.org/) to evaluate model performance on R code generation tasks.
* We tested each model on a shared benchmark: the `are` dataset ("**A**n **R** **E**val"). `are` contains a collection of difficult R coding problems and a column, `target`, with information about the target solution.  
* Using vitals, we had each model solve each problem in `are`. Then, we scored their solutions using a scoring model (Claude 3.7 Sonnet). Each solution received either an Incorrect, Partially Correct, or Correct score. 

You can see all the code used to evaluate the models [here](https://github.com/skaltman/model-eval/tree/main/eval). If you'd like to see a more in-depth analysis, check out Simon Couch's series of [blog posts](https://www.simonpcouch.com/blog/), which this post is based on, including [Claude 4 and R Coding](https://www.simonpcouch.com/blog/2025-05-27-claude-4/). 
